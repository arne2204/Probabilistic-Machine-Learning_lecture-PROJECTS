{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47c05022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "df = pd.read_csv('GOOGL_historical_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f66df73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Daten vorbereiten\n",
    "# -----------------------------\n",
    "# Erwartet: df mit Spalten 'Date' und 'Close'\n",
    "assert 'Close' in df.columns, \"DataFrame df muss die Spalte 'Close' enthalten.\"\n",
    "\n",
    "close = df['Close'].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# Parameter\n",
    "look_back = 10         # Fensterlänge\n",
    "split_percent = 0.8    # 80/20 Split\n",
    "split_idx = int(len(close) * split_percent)\n",
    "\n",
    "# Skalierung: NUR auf Train fitten\n",
    "scaler = MinMaxScaler()\n",
    "close_train = scaler.fit_transform(close[:split_idx])\n",
    "close_test  = scaler.transform(close[split_idx:])\n",
    "\n",
    "def make_windows(arr, L):\n",
    "    X, y = [], []\n",
    "    for i in range(len(arr) - L):\n",
    "        X.append(arr[i:i+L, 0])   # Sequenz der Länge L\n",
    "        y.append(arr[i+L, 0])     # nächster Wert\n",
    "    X = np.array(X, dtype=np.float32)  # [N, L]\n",
    "    y = np.array(y, dtype=np.float32).reshape(-1, 1)  # [N, 1]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = make_windows(close_train, look_back)\n",
    "X_test,  y_test  = make_windows(close_test,  look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f43f91e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Torch Datasets / Loader\n",
    "# -----------------------------\n",
    "class WindowDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "train_ds = WindowDataset(X_train, y_train)\n",
    "test_ds  = WindowDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=False)  # Zeitreihen: kein Shuffle\n",
    "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "634f0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) BayesianLinear-Schicht\n",
    "#    (Blundell et al., 2015)\n",
    "# -----------------------------\n",
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, prior_sigma=1.0):\n",
    "        super().__init__()\n",
    "        # Variational Posterior-Parameter (Gewichte)\n",
    "        self.w_mu   = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "        self.w_rho  = nn.Parameter(torch.full((out_features, in_features), -3.0))  # kleine Std am Start\n",
    "        # Variational Posterior-Parameter (Bias)\n",
    "        self.b_mu   = nn.Parameter(torch.zeros(out_features))\n",
    "        self.b_rho  = nn.Parameter(torch.full((out_features,), -3.0))\n",
    "        # Prior (fix)\n",
    "        self.prior_sigma = prior_sigma\n",
    "        self.prior = torch.distributions.Normal(loc=0.0, scale=prior_sigma)\n",
    "        # Standard-Normal für Sampling\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    def _softplus(self, x):\n",
    "        return torch.log1p(torch.exp(x))  # numerisch stabiler als exp direkt\n",
    "\n",
    "    def sample_weights(self):\n",
    "        w_sigma = self._softplus(self.w_rho)\n",
    "        b_sigma = self._softplus(self.b_rho)\n",
    "        eps_w = self.normal.sample(self.w_mu.shape).to(self.w_mu.device)\n",
    "        eps_b = self.normal.sample(self.b_mu.shape).to(self.b_mu.device)\n",
    "        w = self.w_mu + w_sigma * eps_w\n",
    "        b = self.b_mu + b_sigma * eps_b\n",
    "        return w, b, w_sigma, b_sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Weight-Sampling via Reparametrisierung\n",
    "        w, b, w_sigma, b_sigma = self.sample_weights()\n",
    "        out = x @ w.t() + b  # [B, out_features]\n",
    "        # KL-Divergenz q||p für Gewichte + Bias (Gaussian-Gaussian, closed form)\n",
    "        # KL(N(μ,σ)||N(0,σ0)) = log(σ0/σ) + (σ^2 + μ^2)/(2σ0^2) - 0.5\n",
    "        prior_var = self.prior_sigma ** 2\n",
    "        w_var = w_sigma**2\n",
    "        b_var = b_sigma**2\n",
    "        kl_w = torch.log(self.prior_sigma / w_sigma).sum() + 0.5 * ((w_var + self.w_mu**2) / prior_var - 1).sum()\n",
    "        kl_b = torch.log(self.prior_sigma / b_sigma).sum() + 0.5 * ((b_var + self.b_mu**2) / prior_var - 1).sum()\n",
    "        kl = kl_w + kl_b\n",
    "        return out, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40f60d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) BNN-Modell: MLP auf Fenstern\n",
    "#    (einfach, robust, unabhängig von LSTM)\n",
    "# -----------------------------\n",
    "class BNN(nn.Module):\n",
    "    def __init__(self, L, hidden=64, prior_sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = BayesianLinear(L, hidden, prior_sigma)\n",
    "        self.act = nn.ReLU()\n",
    "        self.fc2 = BayesianLinear(hidden, 1, prior_sigma)\n",
    "        # homoskedastische Beobachtungs-Std (lernbares, positives Skalar)\n",
    "        self.log_sigma_y = nn.Parameter(torch.tensor(-2.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L]\n",
    "        h, kl1 = self.fc1(x)\n",
    "        h = self.act(h)\n",
    "        mu, kl2 = self.fc2(h)   # prädiktiver Mittelwert\n",
    "        kl = kl1 + kl2\n",
    "        sigma_y = torch.nn.functional.softplus(self.log_sigma_y) + 1e-6\n",
    "        return mu, sigma_y, kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61d9a375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | ELBO: 0.8240 | NLL: 0.3514 | KL: 1932.9593\n",
      "Epoch  10 | ELBO: -0.5722 | NLL: -1.0426 | KL: 1923.7362\n",
      "Epoch  20 | ELBO: -0.6189 | NLL: -1.0873 | KL: 1915.5762\n",
      "Epoch  30 | ELBO: -0.7923 | NLL: -1.2590 | KL: 1908.4499\n",
      "Epoch  40 | ELBO: -0.8421 | NLL: -1.3073 | KL: 1902.3488\n",
      "Epoch  50 | ELBO: -1.1261 | NLL: -1.5891 | KL: 1893.8408\n",
      "Epoch  60 | ELBO: -1.0625 | NLL: -1.5228 | KL: 1882.5554\n",
      "Epoch  70 | ELBO: -1.1869 | NLL: -1.6434 | KL: 1867.4292\n",
      "Epoch  80 | ELBO: -1.3872 | NLL: -1.8400 | KL: 1851.8745\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5) Training (ELBO)\n",
    "# -----------------------------\n",
    "model = BNN(L=look_back, hidden=64, prior_sigma=1.0).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def elbo_step(x, y, N):\n",
    "    # Vorwärts: stochastisch wegen Weight-Sampling\n",
    "    mu, sigma_y, kl = model(x)\n",
    "    # Negative Log-Likelihood unter Normal(mu, sigma_y)\n",
    "    nll = 0.5*torch.log(2*math.pi*sigma_y**2) + 0.5*((y - mu)**2)/(sigma_y**2)\n",
    "    nll = nll.mean()                         # Mittel über Batch\n",
    "    elbo = nll + kl / N                      # KL/N (Blundell et al.)\n",
    "    return elbo, nll.item(), kl.item(), sigma_y.item()\n",
    "\n",
    "N = len(train_ds)\n",
    "epochs = 80\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_elbo, total_nll, total_kl = 0.0, 0.0, 0.0\n",
    "    for xb, yb in train_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optim.zero_grad()\n",
    "        elbo, nll_val, kl_val, _ = elbo_step(xb, yb, N)\n",
    "        elbo.backward()\n",
    "        optim.step()\n",
    "        total_elbo += elbo.item() * len(xb)\n",
    "        total_nll  += nll_val     * len(xb)\n",
    "        total_kl   += kl_val      * len(xb)\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:3d} | ELBO: {total_elbo/N:.4f} | NLL: {total_nll/N:.4f} | KL: {total_kl/N:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcb447d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6) MC-Inferenz (T Vorwärtsläufe)\n",
    "# -----------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Xte = torch.from_numpy(X_test).to(device)\n",
    "    yte = torch.from_numpy(y_test).to(device)\n",
    "\n",
    "    T = 100  # Anzahl MC-Samples\n",
    "    mu_samples = []\n",
    "    sigma_obs = None\n",
    "\n",
    "    for _ in range(T):\n",
    "        mu, sigma_y, _ = model(Xte)\n",
    "        mu_samples.append(mu.cpu().numpy())\n",
    "        sigma_obs = sigma_y.item()  # homoskedastisch: identisch in allen Läufen\n",
    "\n",
    "    mu_samples = np.stack(mu_samples, axis=0).squeeze(-1)   # [T, N]\n",
    "    mu_mean_scaled = mu_samples.mean(axis=0, keepdims=True).T  # [N,1]\n",
    "    epistemic_var = mu_samples.var(axis=0, ddof=1, keepdims=True).T  # [N,1]\n",
    "    aleatoric_var = (sigma_obs**2) * np.ones_like(mu_mean_scaled)     # [N,1]\n",
    "    pred_std_scaled = np.sqrt(epistemic_var + aleatoric_var)\n",
    "\n",
    "    # Intervalle (gaussian approx)\n",
    "    q025 = mu_mean_scaled - 1.96 * pred_std_scaled\n",
    "    q975 = mu_mean_scaled + 1.96 * pred_std_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1566128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 BNN (Bayes-by-Backprop) Performance:\n",
      "RMSE: 5.0141   |  Rel. RMSE: 3.88%\n",
      "MAE:  4.0707   |  Rel. MAE:  3.15%\n",
      "MAPE: 3.20%\n",
      "R²:   0.9622\n",
      "95%-Coverage: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# -----------------------------\n",
    "# 7) Rückskalierung & Kennzahlen\n",
    "# -----------------------------\n",
    "y_test_inv = scaler.inverse_transform(y_test)\n",
    "y_pred_inv = scaler.inverse_transform(mu_mean_scaled)\n",
    "q025_inv   = scaler.inverse_transform(q025)\n",
    "q975_inv   = scaler.inverse_transform(q975)\n",
    "\n",
    "# Fehlermaße\n",
    "rmse = math.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "mae  = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "r2   = r2_score(y_test_inv, y_pred_inv)\n",
    "\n",
    "# Relative Fehler\n",
    "mean_price = np.mean(y_test_inv)\n",
    "rel_rmse = rmse / mean_price * 100\n",
    "rel_mae = mae / mean_price * 100\n",
    "mape = np.mean(np.abs((y_test_inv - y_pred_inv) / y_test_inv)) * 100\n",
    "\n",
    "# Coverage 95%\n",
    "coverage95 = np.mean((y_test_inv[:,0] >= q025_inv[:,0]) & (y_test_inv[:,0] <= q975_inv[:,0]))\n",
    "\n",
    "# Ausgabe\n",
    "print(\"\\n📊 BNN (Bayes-by-Backprop) Performance:\")\n",
    "print(f\"RMSE: {rmse:.4f}   |  Rel. RMSE: {rel_rmse:.2f}%\")\n",
    "print(f\"MAE:  {mae:.4f}   |  Rel. MAE:  {rel_mae:.2f}%\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "print(f\"95%-Coverage: {coverage95:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948e385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea29f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Ergebnisse wurden gespeichert in: BNN_Performance3.docx\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Ergebnisse in Word-Dokument speichern\n",
    "# -----------------------------\n",
    "\n",
    "# Rücktransformation der Vorhersagen & Targets\n",
    "y_test_inv = scaler.inverse_transform(y_test)\n",
    "mu_mean_inv = scaler.inverse_transform(mu_mean_scaled)\n",
    "\n",
    "# Hilfsfunktion für Metriken\n",
    "def metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rel_rmse = rmse / np.mean(y_true) * 100\n",
    "    rel_mae  = mae / np.mean(y_true) * 100\n",
    "    return mse, rmse, rel_rmse, mae, rel_mae, mape, r2\n",
    "\n",
    "# Datumsspalte auf Testsplit ausrichten\n",
    "date_test = pd.to_datetime(df['Date'].values[split_idx+look_back:])\n",
    "\n",
    "# === Zeiträume definieren ===\n",
    "periods = {\n",
    "    \"Gesamter Testsplit\": (\"2020-01-01\", date_test.max()),\n",
    "    \"Finanzkrise 2008\": (\"2008-09-01\", \"2009-06-30\"),\n",
    "    \"Corona-Krise\": (\"2020-02-01\", \"2020-12-31\"),\n",
    "    \"Russland-Ukraine Krieg\": (\"2022-02-24\", \"2022-06-30\"),\n",
    "    \"Stabiler Aufwärtstrend\": (\"2017-01-01\", \"2017-12-31\")\n",
    "}\n",
    "\n",
    "# Neues Dokument\n",
    "doc = Document()\n",
    "doc.add_heading(\"Bayesian Neural Network Performance Ergebnisse\", level=1)\n",
    "\n",
    "# === Ergebnisse berechnen & ins Dokument schreiben ===\n",
    "for name, (start, end) in periods.items():\n",
    "    mask = (date_test >= pd.to_datetime(start)) & (date_test <= pd.to_datetime(end))\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    y_true = y_test_inv[mask]\n",
    "    y_pred = mu_mean_inv[mask]\n",
    "    mse, rmse, rel_rmse, mae, rel_mae, mape, r2 = metrics(y_true, y_pred)\n",
    "\n",
    "    doc.add_heading(f\"{name}\", level=2)\n",
    "    doc.add_paragraph(f\"MSE: {mse:.4f}\")\n",
    "    doc.add_paragraph(f\"RMSE: {rmse:.4f}  |  Rel. RMSE: {rel_rmse:.2f}%\")\n",
    "    doc.add_paragraph(f\"MAE: {mae:.4f}   |  Rel. MAE: {rel_mae:.2f}%\")\n",
    "    doc.add_paragraph(f\"MAPE: {mape:.2f}%\")\n",
    "    doc.add_paragraph(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# Dokument speichern\n",
    "output_path = \"BNN_Performance3.docx\"\n",
    "doc.save(output_path)\n",
    "print(f\"\\n✅ Ergebnisse wurden gespeichert in: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b5dc28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Ergebnisse wurden gespeichert in: BNN_Performance_AllPeriods2.docx\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Ergebnisse in Word-Dokument speichern (GANZER Datensatz)\n",
    "# -----------------------------\n",
    "\n",
    "# Rücktransformation des gesamten Datensatzes\n",
    "close_inv = scaler.inverse_transform(close)\n",
    "\n",
    "# Alle Fenster + Targets für den GANZEN Datensatz erzeugen\n",
    "X_all, y_all = make_windows(scaler.transform(close), look_back)\n",
    "\n",
    "# In Torch-Format\n",
    "X_all_torch = torch.from_numpy(X_all).to(device)\n",
    "\n",
    "# MC-Inferenz auf gesamten Datensatz\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    T = 100  # Anzahl MC-Samples\n",
    "    mu_samples = []\n",
    "    sigma_obs = None\n",
    "\n",
    "    for _ in range(T):\n",
    "        mu, sigma_y, _ = model(X_all_torch)\n",
    "        mu_samples.append(mu.cpu().numpy())\n",
    "        sigma_obs = sigma_y.item()\n",
    "\n",
    "    mu_samples = np.stack(mu_samples, axis=0).squeeze(-1)  # [T, N]\n",
    "    mu_mean_scaled = mu_samples.mean(axis=0, keepdims=True).T  # [N,1]\n",
    "    epistemic_var = mu_samples.var(axis=0, ddof=1, keepdims=True).T\n",
    "    aleatoric_var = (sigma_obs**2) * np.ones_like(mu_mean_scaled)\n",
    "    pred_std_scaled = np.sqrt(epistemic_var + aleatoric_var)\n",
    "\n",
    "# Rücktransformation in echte Skala\n",
    "y_all_inv = scaler.inverse_transform(y_all)\n",
    "mu_mean_inv = scaler.inverse_transform(mu_mean_scaled)\n",
    "\n",
    "# Datumsspalte für GANZEN Datensatz\n",
    "date_all = pd.to_datetime(df['Date'].values[look_back:])\n",
    "\n",
    "# Hilfsfunktion für Metriken\n",
    "def metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rel_rmse = rmse / np.mean(y_true) * 100\n",
    "    rel_mae  = mae / np.mean(y_true) * 100\n",
    "    return mse, rmse, rel_rmse, mae, rel_mae, mape, r2\n",
    "\n",
    "# === Zeiträume definieren ===\n",
    "periods = {\n",
    "    \"Gesamter Zeitraum\": (\"2020-01-01\", date_all.max()),\n",
    "    \"Finanzkrise 2008\": (\"2008-09-01\", \"2009-06-30\"),\n",
    "    \"Corona-Krise\": (\"2020-02-01\", \"2020-12-31\"),\n",
    "    \"Russland-Ukraine Krieg\": (\"2022-02-24\", \"2022-06-30\"),\n",
    "    \"Stabiler Aufwärtstrend\": (\"2017-01-01\", \"2017-12-31\")\n",
    "}\n",
    "\n",
    "# Neues Dokument\n",
    "doc = Document()\n",
    "doc.add_heading(\"Bayesian Neural Network Performance Ergebnisse\", level=1)\n",
    "\n",
    "# === Ergebnisse berechnen & ins Dokument schreiben ===\n",
    "for name, (start, end) in periods.items():\n",
    "    mask = (date_all >= pd.to_datetime(start)) & (date_all <= pd.to_datetime(end))\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    y_true = y_all_inv[mask]\n",
    "    y_pred = mu_mean_inv[mask]\n",
    "    mse, rmse, rel_rmse, mae, rel_mae, mape, r2 = metrics(y_true, y_pred)\n",
    "\n",
    "    doc.add_heading(f\"{name}\", level=2)\n",
    "    doc.add_paragraph(f\"MSE: {mse:.4f}\")\n",
    "    doc.add_paragraph(f\"RMSE: {rmse:.4f}  |  Rel. RMSE: {rel_rmse:.2f}%\")\n",
    "    doc.add_paragraph(f\"MAE: {mae:.4f}   |  Rel. MAE: {rel_mae:.2f}%\")\n",
    "    doc.add_paragraph(f\"MAPE: {mape:.2f}%\")\n",
    "    doc.add_paragraph(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# Dokument speichern\n",
    "output_path = \"BNN_Performance_AllPeriods2.docx\"\n",
    "doc.save(output_path)\n",
    "print(f\"\\n✅ Ergebnisse wurden gespeichert in: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
