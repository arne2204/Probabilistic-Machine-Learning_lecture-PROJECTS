{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeaad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eingabe Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f754c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "längeAnalyse= 10\n",
    "\n",
    "startZeit= pd.to_datetime(\"2014-01-01 00:00\")\n",
    "endZeit= pd.to_datetime(\"2023-12-31 23:00\")\n",
    "\n",
    "#10% unproblematisch, 30% warscheinlich zuviel\n",
    "maxShareMissingValues= 0.1\n",
    "\n",
    "features= ['TT_TU', 'RF_TU', '  R1', '  P0', '   F']\n",
    "stationFeatures= ['MESS_DATUM', 'STATIONS_ID', 'Stationshoehe', 'geoBreite', 'geoLaenge', 'Stationsname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47279fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#realistische Wertebereich\n",
    "\n",
    "#Lufttemperatur -38 -- 45 ('TT_TU')\n",
    "#Relative Luftfeuchtigkeit 0 -- 100 ('RF_TU')\n",
    "#Niederschlagshöhe 0 -- 245 ('  R1')\n",
    "#Niederschlags Indikator 0 -- 1 ('RS_IND')\n",
    "#Niederschlags Form 0 -- 9 ('WRTR')\n",
    "#Luftdruck Meereshöhe 800 -- 1100 ('   P')\n",
    "#Luftdruck 800 -- 1100 ('  P0')\n",
    "#Windgeschwindigkeit 0 -- 350 ('   F')\n",
    "#Windrichtung 0 -- 360 ('   D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "werteBereicheDictionary= {\n",
    "    'TT_TU': [-38, 45],\n",
    "    'RF_TU': [0, 100],\n",
    "    '  R1': [0, 245],\n",
    "    'RS_IND': [0, 1],\n",
    "    'WRTR': [0, 9],\n",
    "    '   P': [800, 1100],\n",
    "    '  P0': [800, 1100],\n",
    "    '   F': [0, 350],\n",
    "    '   D': [0, 360]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73141f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation Methoden\n",
    "#                           1h - 3h         3h - 1 d                    1d - 3d                     > 3d\n",
    "#TT_TU(Tempreature)         Forward Fill    Zeitbasierte Interpolation  Zeitbasierte Interpolation  Lineare Regression\n",
    "#RF_TU(Relative Humidity)   Forward Fill    Zeitbasierte Interpolation  Zeitbasierte Interpolation  Random Forest Imputation\n",
    "#R1(Percipation)            FillNA(0)       Zeitbasierte Interpolation  Saisonale Mittelwerte       Lineare Regression\n",
    "#P0(Presure)                Forward Fill    Zeitbasierte Interpolation  Zeitbasierte Interpolation  Lineare Regression\n",
    "#F(Windspeed)               Forward Fill    Zeitbasierte Interpolation  Zeitbasierte Interpolation  Random Forest Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#independendMeasurments= [(\"TT_TU\", 1, 3, \"Forward Fill\"), (\"RF_TU\", 1, 3, \"Forward Fill\"), (\"  R1\", 1, 3, \"FillNA(0)\"), (\"  P0\", 1, 3, \"Forward Fill\"), (\"   F\", 1, 3, \"Forward Fill\"), (\"TT_TU\", 4, 72, \"Zeitbasierte Interpolation\"), (\"RF_TU\", 4, 72, \"Zeitbasierte Interpolation\"), (\"  R1\", 4, 24, \"Zeitbasierte Interpolation\"), (\"  P0\", 4, 72, \"Zeitbasierte Interpolation\"), (\"   F\", 4, 72, \"Zeitbasierte Interpolation\"), (\"  R1\", 25, 72, \"Saisonale Mittelwerte\")]\n",
    "#dependendMeasurments= [(\"TT_TU\", 73, -1, \"Lineare Regression\"), (\"RF_TU\", 73, -1, \"Random Forest Imputation\"), (\"  R1\", 73, -1, \"Lineare Regression\"), (\"  P0\", 73, -1, \"Lineare Regression\"), (\"   F\", 73, -1, \"Random Forest Imputation\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Files Einlesen (Dictionary pro Typ und Geodaten einzeln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5cb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"..\\data\\TU\"\n",
    "\n",
    "txt_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "dataframesTU = {}\n",
    "\n",
    "for file_name in txt_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    df = pd.read_csv(file_path, sep= \";\")\n",
    "\n",
    "    #Name creation\n",
    "    file_base = os.path.splitext(file_name)[0]\n",
    "    parts = file_base.split(\"_\")\n",
    "    key = f\"{parts[1]}_{parts[-1]}\"\n",
    "\n",
    "    dataframesTU[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2dd5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"..\\data\\RR\"\n",
    "\n",
    "txt_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "dataframesRR = {}\n",
    "\n",
    "for file_name in txt_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    df = pd.read_csv(file_path, sep= \";\")\n",
    "\n",
    "    #Name creation\n",
    "    file_base = os.path.splitext(file_name)[0]\n",
    "    parts = file_base.split(\"_\")\n",
    "    key = f\"{parts[1]}_{parts[-1]}\"\n",
    "\n",
    "    dataframesRR[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"..\\data\\P0\"\n",
    "\n",
    "txt_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "dataframesP0 = {}\n",
    "\n",
    "for file_name in txt_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    df = pd.read_csv(file_path, sep= \";\")\n",
    "\n",
    "    #Name creation\n",
    "    file_base = os.path.splitext(file_name)[0]\n",
    "    parts = file_base.split(\"_\")\n",
    "    key = f\"{parts[1]}_{parts[-1]}\"\n",
    "\n",
    "    dataframesP0[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822286fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"..\\data\\FF\"\n",
    "\n",
    "txt_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "\n",
    "dataframesFF = {}\n",
    "\n",
    "for file_name in txt_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    df = pd.read_csv(file_path, sep= \";\")\n",
    "\n",
    "    #Name creation\n",
    "    file_base = os.path.splitext(file_name)[0]\n",
    "    parts = file_base.split(\"_\")\n",
    "    key = f\"{parts[1]}_{parts[-1]}\"\n",
    "\n",
    "    dataframesFF[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDictionary= {\n",
    "    \"TU\": dataframesTU,\n",
    "    \"RR\": dataframesRR,\n",
    "    \"P0\": dataframesP0,\n",
    "    \"FF\": dataframesFF\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aacb4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = [\"Stations_id\", \"von_datum\", \"bis_datum\", \"Stationshoehe\",\"geoBreite\", \"geoLaenge\", \"Stationsname\", \"Bundesland\", \"Abgabe\"]\n",
    "\n",
    "methaDataTUtxt= pd.read_fwf(\"..\\data\\Metha List\\TU_Stundenwerte_Beschreibung_Stationen.txt\", encoding=\"iso-8859-1\", skiprows=2, names=columnNames)\n",
    "\n",
    "methaDataTU= pd.DataFrame(methaDataTUtxt)\n",
    "\n",
    "methaDataTU= methaDataTU.drop(columns= [\"von_datum\", \"bis_datum\", \"Bundesland\", \"Abgabe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c69e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datatype conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataKey, dataFrames in dataDictionary.items():\n",
    "    for key, frames in dataDictionary[dataKey].items():\n",
    "        dataDictionary[dataKey][key]['MESS_DATUM']= pd.to_datetime(frames['MESS_DATUM'].astype(str), format= '%Y%m%d%H')\n",
    "        dataDictionary[dataKey][key] = dataDictionary[dataKey][key].sort_values('MESS_DATUM')\n",
    "\n",
    "        dataDictionary[dataKey][key]= dataDictionary[dataKey][key].drop('eor', axis= 1)\n",
    "        dataDictionary[dataKey][key]= dataDictionary[dataKey][key].drop(columns=[col for col in dataDictionary[dataKey][key].columns if col.startswith(\"QN_\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b541ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zu vollen Timeline auffüllen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeFrame= pd.date_range(start= startZeit, end= endZeit, freq='H')\n",
    "dfRange= pd.DataFrame({'MESS_DATUM': timeFrame})\n",
    "\n",
    "for dataKey, dataFrames in dataDictionary.items():\n",
    "    for key, frames in dataDictionary[dataKey].items():\n",
    "        dataDictionary[dataKey][key]= dfRange.merge(frames, on= 'MESS_DATUM', how= 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9398f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataKey, dataFrames in dataDictionary.items():\n",
    "    for key, frames in dataDictionary[dataKey].items():\n",
    "        print(key, frames.isnull().any(axis=1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pro Station Mergen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a468f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationGroups= defaultdict(list)\n",
    "\n",
    "for dataKey, dataFrames in dataDictionary.items():\n",
    "    for key, frames in dataDictionary[dataKey].items():\n",
    "        stationId= key.split(\"_\")[1]\n",
    "        stationGroups[stationId].append((dataKey, frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationDataDictionary= {}\n",
    "\n",
    "for stationId, framesList in stationGroups.items():\n",
    "    dfMerged= None\n",
    "\n",
    "    for key, frames in framesList:\n",
    "        dfRename= frames.copy()\n",
    "        dfRename= dfRename.rename(columns= lambda col: f\"{key}_{col}\" if col == \"STATIONS_ID\" and not key == \"FF\" else col)\n",
    "\n",
    "        if dfMerged is None:\n",
    "            dfMerged= dfRename\n",
    "        else:\n",
    "            dfMerged=pd.merge(dfMerged, dfRename, on= 'MESS_DATUM', how= 'outer')\n",
    "    \n",
    "    cols= list(dfMerged.columns)\n",
    "    cols.remove(\"STATIONS_ID\")\n",
    "    cols.insert(1, \"STATIONS_ID\")\n",
    "    stationDataDictionary[stationId]= dfMerged[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5708a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, frames in stationDataDictionary.items():\n",
    "    stationIdColsToDrop = [\n",
    "        col for col in stationDataDictionary[key].columns\n",
    "        if col.endswith('_STATIONS_ID')\n",
    "    ]\n",
    "    stationDataDictionary[key] = stationDataDictionary[key].drop(columns=stationIdColsToDrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f75969",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, frames in stationDataDictionary.items():\n",
    "    stationDataDictionary[key]['STATIONS_ID']= stationDataDictionary[key]['STATIONS_ID'].mode().iloc[0]\n",
    "\n",
    "    stationDataDictionary[key]= pd.merge(stationDataDictionary[key], methaDataTU, left_on= 'STATIONS_ID', right_on= 'Stations_id', how= 'left')\n",
    "    stationDataDictionary[key]= stationDataDictionary[key].drop(columns= [\"Stations_id\"], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, frame in stationDataDictionary.items():\n",
    "    stationDataDictionary[key]['hour'] = frame['MESS_DATUM'].dt.hour\n",
    "    stationDataDictionary[key]['day'] = frame['MESS_DATUM'].dt.day\n",
    "    stationDataDictionary[key]['month'] = frame['MESS_DATUM'].dt.month\n",
    "    stationDataDictionary[key]['hour_sin'] = np.sin(2 * np.pi * frame['hour'] / 24)\n",
    "    stationDataDictionary[key]['hour_cos'] = np.cos(2 * np.pi * frame['hour'] / 24)\n",
    "    stationDataDictionary[key]['month_sin'] = np.sin(2 * np.pi * frame['month'] / 12)\n",
    "    stationDataDictionary[key]['month_cos'] = np.cos(2 * np.pi * frame['month'] / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, frame in stationDataDictionary.items():\n",
    "    print(key, frame.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stationDataDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9490975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unrealistische Werte als Missingvalues deklarieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e14ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, frame in stationDataDictionary.items():\n",
    "    for spalte, (min_wert, max_wert) in werteBereicheDictionary.items():\n",
    "        if spalte in frame.columns:\n",
    "            stationDataDictionary[key][spalte] = frame[spalte].where((frame[spalte] >= min_wert) & (frame[spalte] <= max_wert), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stationen mit zuvielen Missing rausfiltern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, frame in stationDataDictionary.items():\n",
    "    for col in stationFeatures:\n",
    "        if col in frame.columns:\n",
    "            missingIndices = frame[frame[col].isna()].index.tolist()\n",
    "            if missingIndices:\n",
    "                print(f\"Spalte '{col}' hat {len(missingIndices)} fehlende Werte an den Zeilen:\")\n",
    "                print(missingIndices)\n",
    "                del stationDataDictionary[key]\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Spalte '{col}' ist nicht im DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149894b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keysToDelete = []\n",
    "missingValueDictionary= {}\n",
    "\n",
    "for key, frame in stationDataDictionary.items():\n",
    "    print(f\"\\n--- Analyse für Station: {key} ---\")\n",
    "    for col in features:\n",
    "        if col in frame.columns:\n",
    "            totalLen = len(frame)\n",
    "            missingMask = frame[col].isna()\n",
    "            numMissing = missingMask.sum()\n",
    "            \n",
    "            if numMissing > 0:\n",
    "                missingPct = (numMissing / totalLen) * 100\n",
    "\n",
    "                # Zusammenhängende NaN-Blöcke zählen\n",
    "                gaps = []\n",
    "                count = 0\n",
    "                for val in missingMask:\n",
    "                    if val:\n",
    "                        count += 1\n",
    "                    elif count > 0:\n",
    "                        gaps.append(count)\n",
    "                        count = 0\n",
    "                if count > 0:\n",
    "                    gaps.append(count)\n",
    "\n",
    "                gaps_sorted = sorted(gaps, reverse=True)\n",
    "\n",
    "                print(f\"\\nSpalte '{col}':\")\n",
    "                print(f\"- Fehlend: {numMissing} von {totalLen} Werten ({missingPct:.2f}%)\")\n",
    "                print(f\"- Alle Längen (absteigend): {gaps_sorted}\")\n",
    "                print(f\"- Anzahl Lücken: {len(gaps_sorted)}\")\n",
    "\n",
    "                if missingPct > maxShareMissingValues * 100:\n",
    "                    print(\"\\n🚨🚨🚨 STATION GELÖSCHT wegen zu vielen fehlenden Werten! 🚨🚨🚨\")\n",
    "                    keysToDelete.append(key)\n",
    "                    break\n",
    "\n",
    "                missingValueDictionary[str(key) + str(col)]= (key, col, missingPct, gaps_sorted)\n",
    "            else:\n",
    "                print(f\"\\nSpalte '{col}' hat keine fehlenden Werte.\")\n",
    "        else:\n",
    "            print(f\"\\nSpalte '{col}' ist nicht im DataFrame '{key}'.\")\n",
    "\n",
    "for key in keysToDelete:\n",
    "    del stationDataDictionary[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe Compination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a42e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rowwise Compination\n",
    "rowwiseDf = pd.concat(stationDataDictionary.values(), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d70f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columnwise Compination\n",
    "colwiseDf = pd.concat(stationDataDictionary, axis=1)\n",
    "colwiseDf.columns = [f\"{col}_{station}\" for station, col in colwiseDf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe554d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colwiseDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed01b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45ad4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\data\\stationDataDictionary.pkl\", \"wb\") as f:\n",
    "    pickle.dump(stationDataDictionary, f)\n",
    "\n",
    "with open(\"..\\data\\missingValueDictionary.pkl\", \"wb\") as f:\n",
    "    pickle.dump(missingValueDictionary, f)\n",
    "\n",
    "rowwiseDf.to_pickle(r\"..\\data\\rowwiseDf.pkl\")\n",
    "colwiseDf.to_pickle(r\"..\\data\\colwiseDf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7657f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10292b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"missingSectionDictionay= {}\n",
    "\n",
    "for key, frame in stationDataDictionary.items():\n",
    "    missing= []\n",
    "\n",
    "    for col in features:\n",
    "        row= 0\n",
    "        count= 0\n",
    "        tmp= -1\n",
    "\n",
    "        while row < len(frame[col]):\n",
    "            if pd.isna(frame[col][row]):\n",
    "                count+= 1\n",
    "                if tmp == -1:\n",
    "                    tmp= row\n",
    "            elif tmp != -1:\n",
    "                missing.append((col, row, count))\n",
    "                count= 0\n",
    "                tmp= -1\n",
    "            else:\n",
    "                count= 0\n",
    "            \n",
    "            row+= 1\n",
    "        \n",
    "        if count != 0:\n",
    "            missing.append((col, row, count))\n",
    "\n",
    "    missingSectionDictionay[key]= missing\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"independendImpulation= {}\n",
    "dependendImpulation= {}\n",
    "\n",
    "for key, liste in missingSectionDictionay.items():\n",
    "    independendGapList= []\n",
    "    dependendGapList= []\n",
    "    for gap in liste:\n",
    "        for measurment in independendMeasurments:\n",
    "            if (gap[0] == measurment[0] and gap[2] >= measurment[1] and (gap[2] <= measurment[2] or measurment[2] == -1)):\n",
    "                independendGapList.append((gap[0], gap[1], gap[2], measurment[3]))\n",
    "    \n",
    "        for measurment in dependendMeasurments:\n",
    "            if (gap[0] == measurment[0] and gap[2] >= measurment[1] and (gap[2] <= measurment[2] or measurment[2] == -1)):\n",
    "                dependendGapList.append((gap[0], gap[1], gap[2], measurment[3]))\n",
    "\n",
    "    independendImpulation[key]= independendGapList\n",
    "    dependendImpulation[key]= dependendGapList\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(independendImpulation)\n",
    "#print(dependendImpulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(independendMeasurments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"for frameKey, frame in stationDataDictionary.items():\n",
    "    for key, liste in independendImpulation.items():\n",
    "        if key != frameKey:\n",
    "            for tripel in liste:\n",
    "                start= tripel[1]\n",
    "                end= tripel[1] + tripel[2]\n",
    "                \n",
    "                if tripel[3] == 'Forward Fill':\n",
    "                    stationDataDictionary[key].loc[start:end, tripel[0]]= stationDataDictionary[key][tripel[0]].ffill()[start:end]\n",
    "\n",
    "                elif tripel[3] == 'FillNA(0)':\n",
    "                    stationDataDictionary[key].loc[start:end, tripel[0]]= 0\n",
    "\n",
    "                elif tripel[3] == 'Zeitbasierte Interpolation':\n",
    "                    stationDataDictionary[key].loc[start:end, tripel[0]]= stationDataDictionary[key][tripel[0]].interpolate(method= 'linear')[start:end]\n",
    "\n",
    "                elif tripel[3] == 'Saisonale Mittelwerte':\n",
    "                    seasonalMeans= stationDataDictionary[key].groupby(['month', 'hour'])[tripel[0]].mean()\n",
    "\n",
    "                    for i in range(start, end + 1):\n",
    "                        m= frame.loc[i, 'month']\n",
    "                        h= frame.loc[i, 'hour']\n",
    "                        meanVal= seasonalMeans.get((m, h), np.nan)\n",
    "\n",
    "                        stationDataDictionary[key].loc[i, tripel[0]]= meanVal\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06db05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"keysToDelete = []\n",
    "\n",
    "for key, frame in stationDataDictionary.items():\n",
    "    print(f\"\\n--- Analyse für Station: {key} ---\")\n",
    "    for col in features:\n",
    "        if col in frame.columns:\n",
    "            totalLen = len(frame)\n",
    "            missingMask = frame[col].isna()\n",
    "            numMissing = missingMask.sum()\n",
    "            \n",
    "            if numMissing > 0:\n",
    "                missingPct = (numMissing / totalLen) * 100\n",
    "\n",
    "                # Zusammenhängende NaN-Blöcke zählen\n",
    "                gaps = []\n",
    "                count = 0\n",
    "                for val in missingMask:\n",
    "                    if val:\n",
    "                        count += 1\n",
    "                    elif count > 0:\n",
    "                        gaps.append(count)\n",
    "                        count = 0\n",
    "                if count > 0:\n",
    "                    gaps.append(count)\n",
    "\n",
    "                gaps_sorted = sorted(gaps, reverse=True)\n",
    "\n",
    "                print(f\"\\nSpalte '{col}':\")\n",
    "                print(f\"- Fehlend: {numMissing} von {totalLen} Werten ({missingPct:.2f}%)\")\n",
    "                print(f\"- Alle Längen (absteigend): {gaps_sorted}\")\n",
    "                print(f\"- Anzahl Lücken: {len(gaps_sorted)}\")\n",
    "\n",
    "                if missingPct > maxShareMissingValues * 100:\n",
    "                    print(\"\\n🚨🚨🚨 STATION GELÖSCHT wegen zu vielen fehlenden Werten! 🚨🚨🚨\")\n",
    "                    keysToDelete.append(key)\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                print(f\"\\nSpalte '{col}' hat keine fehlenden Werte.\")\n",
    "        else:\n",
    "            print(f\"\\nSpalte '{col}' ist nicht im DataFrame '{key}'.\")\n",
    "\n",
    "for key in keysToDelete:\n",
    "    del stationDataDictionary[key]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15275875",
   "metadata": {},
   "outputs": [],
   "source": [
    "#missingSectionDictionay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
