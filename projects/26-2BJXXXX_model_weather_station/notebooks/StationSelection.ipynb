{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e58228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e330d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    \"Stations_id\", \"von_datum\", \"bis_datum\", \"Stationshoehe\",\n",
    "    \"geoBreite\", \"geoLaenge\", \"Stationsname\", \"Bundesland\", \"Abgabe\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71739693",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTUtxt= pd.read_fwf(\"..\\data\\Metha List\\TU_Stundenwerte_Beschreibung_Stationen.txt\", encoding=\"iso-8859-1\", skiprows=2, names=column_names)\n",
    "dataRRtxt= pd.read_fwf(\"..\\data\\Metha List\\RR_Stundenwerte_Beschreibung_Stationen.txt\", encoding=\"iso-8859-1\", skiprows=2, names=column_names)\n",
    "dataP0txt= pd.read_fwf(\"..\\data\\Metha List\\P0_Stundenwerte_Beschreibung_Stationen.txt\", encoding=\"iso-8859-1\", skiprows=2, names=column_names)\n",
    "dataFFtxt= pd.read_fwf(\"..\\data\\Metha List\\FF_Stundenwerte_Beschreibung_Stationen.txt\", encoding=\"iso-8859-1\", skiprows=2, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb365d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTU= pd.DataFrame(dataTUtxt)\n",
    "dataRR= pd.DataFrame(dataRRtxt)\n",
    "dataP0= pd.DataFrame(dataP0txt)\n",
    "dataFF= pd.DataFrame(dataFFtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c8770f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Stations_id', 'von_datum', 'bis_datum', 'Stationshoehe', 'geoBreite',\n",
      "       'geoLaenge', 'Stationsname', 'Bundesland', 'Abgabe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dataTU.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cacee450",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set= [dataTU, dataRR, dataP0, dataFF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c33e2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_zeit= pd.to_datetime(\"2014-01-01\")\n",
    "end_zeit= pd.to_datetime(\"2023-12-31\")\n",
    "\n",
    "#vor 1996 keine Daten\n",
    "min_jahr= 1996\n",
    "max_jahr= 2024\n",
    "\n",
    "fenster_längen=[5,6,7,8,9,10]\n",
    "max_distance= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "922580de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_set)):\n",
    "    data_set[i]['von_datum']= pd.to_datetime(data_set[i]['von_datum'].astype(str), format='%Y%m%d')\n",
    "    data_set[i]['bis_datum']= pd.to_datetime(data_set[i]['bis_datum'].astype(str), format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5af64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_set)):\n",
    "    data_set[i]= data_set[i].drop(['Bundesland'], axis= 1)\n",
    "    data_set[i]= data_set[i][data_set[i]['Abgabe'] == 'Frei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c70adec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_orginal= [df.copy() for df in data_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707c4dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for startjahr in range(min_jahr, max_jahr):\n",
    "    fenster_start = pd.Timestamp(f\"{startjahr}-01-01\")\n",
    "    \n",
    "    for länge in fenster_längen:\n",
    "        fenster_ende = fenster_start + pd.DateOffset(years= länge)\n",
    "        if(fenster_ende >= pd.Timestamp(f\"{max_jahr}-01-01\")):\n",
    "            continue\n",
    "\n",
    "        for i in range(len(data_set)):\n",
    "            maske= (data_set[i]['von_datum'] <= fenster_start) & (data_set[i]['bis_datum'] >= fenster_ende)\n",
    "            data_set[i]= data_set[i][maske]    \n",
    "            df_tmp= data_set[0][['Stations_id']].copy()\n",
    "\n",
    "        for i in data_set[1:]:\n",
    "            df_tmp= pd.merge(df_tmp, i['Stations_id'], on= 'Stations_id')\n",
    "\n",
    "        for i in  range(len(data_set)):\n",
    "            data_set[i]= pd.merge(data_set[i], df_tmp, on= 'Stations_id')\n",
    "\n",
    "        assert data_set[i]['Stations_id'].is_unique\n",
    "\n",
    "        nachbarn_set = set()\n",
    "\n",
    "        for i, row in data_set[0].iterrows():\n",
    "            coord1 = (row['geoBreite'], row['geoLaenge'])\n",
    "            station_id = row['Stationsname']\n",
    "            count = 0\n",
    "            nachbarn_id= [row['Stations_id']]\n",
    "\n",
    "            for j, row2 in data_set[0].iterrows():\n",
    "                if i == j:\n",
    "                    continue\n",
    "                coord2 = (row2['geoBreite'], row2['geoLaenge'])\n",
    "                if geodesic(coord1, coord2).km <= max_distance:\n",
    "                    count += 1\n",
    "                    nachbarn_id.append(row2['Stations_id'])\n",
    "    \n",
    "            nachbarn_set.add((station_id, count, tuple(nachbarn_id)))\n",
    "\n",
    "        sortiert = sorted(nachbarn_set, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if sortiert:\n",
    "            results.append({\"startjahr\": startjahr, \"start_zeitpunkt\": fenster_start, \"end_zeitpunkt\": fenster_ende,\"dauer_jahre\": länge, \"anzahl_stationen\": sortiert[0][1], \"Stationsname\": sortiert[0][0], \"Score\": länge*sortiert[0][1], \"Stations_ids\": sortiert[0][2]})\n",
    "        \n",
    "        data_set= [df.copy() for df in data_set_orginal]\n",
    "\n",
    "sortierte_results= sorted(results, key= lambda x: (-x[\"Score\"], -x[\"startjahr\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1a509d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'startjahr': 2013, 'start_zeitpunkt': Timestamp('2013-01-01 00:00:00'), 'end_zeitpunkt': Timestamp('2023-01-01 00:00:00'), 'dauer_jahre': 10, 'anzahl_stationen': 18, 'Stationsname': 'Erfurt-Weimar', 'Score': 180, 'Stations_ids': (1270, 198, 656, 867, 1612, 1691, 2044, 2171, 2261, 2925, 3231, 3513, 3821, 3946, 4464, 4501, 5371, 5490, 7368)}\n"
     ]
    }
   ],
   "source": [
    "print(sortierte_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aaf954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
